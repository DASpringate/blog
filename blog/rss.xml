<?xml version="1.0" encoding="ISO-8859-1" ?><rss version="2.0">
  <channel>
    <title>What is this? David Springate's personal blog</title>
    <link>http://daspringate.github.com</link>
    <description>David Springate's personal blog on programming, data, informatics, biostatistics and evolution</description>
    <category>R</category>
    <category>Clojure</category>
    <category>data</category>
    <category>informatics</category>
    <category>biostatistics</category>
    <item><title>Functional programming in R</title><link>http://daspringate.github.io/posts/2013/05/Functional_programming_in_R.html</link><category>R</category><category>functional-programming</category><category>Lisp</category><category>Closures</category><description>&lt;h1&gt;Functional programming in R&lt;/h1&gt;

&lt;p&gt;&lt;em&gt;This post is based on a talk I gave at the &lt;a href=&quot;http://www.meetup.com/Manchester-R/&quot;&gt;Manchester R User Group&lt;/a&gt; on functional programming in R on May 2nd 2013.  The original slides can be found &lt;a href=&quot;http://www.slideshare.net/DASpringate/functional-programming-in-r&quot;&gt;here&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This post is about functional programming, why it is at the heart of the R language and how it can hopefully help you to write cleaner, faster and more bug-free R programs.
I will discuss what functional programming is at a very abstract level as a means of the representation of some simplified model of reality on a computer.  Then I’ll talk about the elements that functional programming is comprised of and highlight the most important elements in programming in R.  I will then go through a quick example demo of a FP-style generic bootstrap algorithm to sample linear models and return bootstrap confidence intervals.  I’ll compare this with a non-FP alternative version so you will hopefully clearly  see the advantages of using an FP style.  To wrap up, I’ll make a few suggestions for places to go to if you want to learn more about functional programming in R.&lt;/p&gt;

&lt;h2&gt;What is Functional programming?&lt;/h2&gt;

&lt;p&gt;&amp;hellip; Well, what is programming?  When you write a program you are building an  abstract representation of some tiny subset of reality on your computer, whether it is an experiment you have conducted or a model of some financial system or a collection of features of members of a population. There are obviously different ways to represent reality, and the different different methods of doing so programmatically can be thought of as the metaphysics of different styles of programming.&lt;/p&gt;

&lt;p&gt;Consider for a moment building a representation of a river on a computer, a model of a river system for example.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://daspringate.github.io/img/river.jpg&quot; alt=&quot;alt River&quot;/&gt;&lt;/p&gt;

&lt;p&gt;In non-functional languages such as C++, Java and (to some extent) Python, the river is an object in itself, a &lt;code&gt;thing&lt;/code&gt; that does &lt;code&gt;things&lt;/code&gt; to other &lt;code&gt;things&lt;/code&gt; and that may have various properties associated with it such as flow rate, depth and pollution levels.  These properties may change over time but there is always this constant, the river, which somehow persists over time.&lt;/p&gt;

&lt;p&gt;In FP we look at things differently&amp;hellip; &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://daspringate.github.io/img/heraclitus.jpg&quot; alt=&quot;Hereclitus - We never step into the same river twice&quot;/&gt;&lt;/p&gt;

&lt;p&gt;The presocratic philosopher  Hereclitus said “We never step into the same river twice”, recognising that the thing we call a river is not really an object in itself, but something undergoing constant change through a variety of processes. In functional programming we are less concerned with the object of the river itself but rather the processes that it undergoes through time.  Our river at any point in time is just a collection of values (say, particles and their positions). These values then feed into a process to generate the series of values at the next time point.  So we have data flowing through processes of functions and that collection of data over time is what we call a river, which is really defined by the functions that the data flows through.  This is a very different way of looking at things to that of imperative, object oriented programming.&lt;/p&gt;

&lt;p&gt;After this somewhat abstract and philosophical start, I&amp;#39;ll talk about the more practical elements of functional programming (FP).  FP has been around for a very long time and originally  stems from Lisp, which was first implemented in the 1950’s. It is making something of a comeback of late for a variety of reasons, but mostly because it is so good at dealing with concurrent, multicore problems potentially over many computers.  There are several elements that FP is generally considered to be comprised of.  Different languages highlight different elements, depending on how strictly functional they are.&lt;/p&gt;

&lt;h3&gt;Functions are first class citizens of the language&lt;/h3&gt;

&lt;p&gt;This means that functions can be treated just like any other data type - They can be passed around as arguments to other functions, returned by other functions and stored in lists.  This is the really big deal about functional programming and allows for higher-order functions (such as &lt;code&gt;lapply&lt;/code&gt;) and closures, which I&amp;#39;ll talk about later.  This is the most fundamental functional concept and I&amp;#39;d argue that a language has to have this property in order to be called a functional language, even if it has some of the other elements listed below.  For example, Python has anonymous functions and supports declarative programming with list comprehensions and generators, but functions are fundamentally different from data-types such as lists so Python cannot really be described as a functional language in the same way as Scheme or R can be.&lt;/p&gt;

&lt;h3&gt;Functional purity&lt;/h3&gt;

&lt;p&gt;This is more of an element of good functional program design.  Pure functions take arguments, return values and otherwise have no side effects - no I/O or global variable changes.  This means that if you call a pure function twice with the same arguments, you know it will return the same value.  This means programs are easily tested because you can test different elements in isolation and once you know they work, you can treat them like a black box, knowing that they will not change some other part of your code somewhere else.  Some very strictly functional languages, like Haskell, insist on functional purity to the extent that in order to output data or read or write files you are forced to wrap your &amp;#39;dirty&amp;#39; functions in constructs called monads to preserve the purity of your code.  R does not insist on functional purity, but it is often good practice to split your code into pure and impure functions.  This means you can test your pure code easily and confine your I/O and random numbers etc to a small number of dirty functions.&lt;/p&gt;

&lt;h3&gt;Vectorised functions&lt;/h3&gt;

&lt;p&gt;Vectorised functions operate equally well on all elements of a vector as they do on a single number.  They are very important in R programming to the point that much of the criticism of R as a &lt;code&gt;really&lt;/code&gt; slow language can be put down to failing to properly understand vectorisation.  This also includes the declarative style of programming, where you tell the language what you want, rather than how you want to get it.  This is common in languages like SQL and in Python generators.  I&amp;#39;ll discuss this more later.&lt;/p&gt;

&lt;h3&gt;Anonymous functions&lt;/h3&gt;

&lt;p&gt;In FP, naming and applying a function are two separate operations, you don&amp;#39;t need to give your functions names in order to call them. So, calling this function:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;powfun &amp;lt;- function(x, pow) {
    x^pow
}
powfun(2, 10)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] 1024
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;to the interpreter is exactly the same as applying variables to the anonymous function:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;(function(x, pow) {
    x^pow
})(2, 10)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] 1024
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is particularly useful when you are building small, single use functions such as those used as arguments in higher order functions such as &lt;code&gt;lapply&lt;/code&gt;.&lt;/p&gt;

&lt;h3&gt;Immutable data structures&lt;/h3&gt;

&lt;p&gt;Immutable data structures are associated with pure functions. The idea is that once an object such as a vector or list is created, it should not be changed.  You can&amp;#39;t affect your data structures via side effects from other functions. Going back to our river example, doing so would be like going back in time and rearranging some of the molecules and starting again.  Having immutable objects means that you can reason more easily about what is going on in your program. Some languages, like Clojure, only have immutable data structures and it is impossible to change a list in place, you would have to have a list as an argument to a function which returns another list that you then assign back to the variable name for the original list.  R does not insist on immutability, but in general, data structures are only changed in this way and not through side effects.  It is often best to follow this, for the same reasons as it is best to have pure functions.&lt;/p&gt;

&lt;h3&gt;Recursion&lt;/h3&gt;

&lt;p&gt;Recursive functions are functions that call themselves.  Historically, these have been hugely important in FP, to the extent that some languages (for example Scheme) do not even have &lt;code&gt;for&lt;/code&gt; loops and they define all of their looping constructs via recursion.  R does allow for recursive functions and they can sometimes be useful, particularly in traversal of tree-like data structures, but recursion in not very efficient R (it is not &lt;a href=&quot;http://en.wikipedia.org/wiki/Tail-call_optimization&quot;&gt;tail-call optimised&lt;/a&gt;) and I will not discuss it further here, though it may well be the subject of a future post.&lt;/p&gt;

&lt;h2&gt;Functional Programming in R&lt;/h2&gt;

&lt;p&gt;R has a reputation for being an ugly, hacked together and slow language. I think this is slightly unfair, but in this ever-so-slightly biassed account, I am going to blame the parents:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://daspringate.github.io/img/R_genealogy.png&quot; alt=&quot;R genealogy&quot;/&gt;&lt;/p&gt;

&lt;p&gt;R is the offspring of the languages &lt;code&gt;S&lt;/code&gt; and &lt;code&gt;Scheme&lt;/code&gt;.  S is a statistical language invented in the 1970&amp;#39;s which is itself based on non-functional, imperative languages like C and Fortran.  It is useful in this domain and much of R&amp;#39;s statistical abilities stem from this, but it is certainly less than pretty.  Scheme is a concise, elegant, functional language in the lisp family. The designers of R tried to build something with the statistical functionality of S and the elegance of Scheme.  Unfortunately, they left in much of the inelegant stuff from S as well and this mixed parentage means that it is now perfectly possible to write ugly, hacky, slow code in the style of S, just as it is also possible to write elegant, efficient functional code in the style of scheme.  The problem is that functional programming has been far less mainstream so people tend to learn to code in the way they know first, resulting in rafts of ugly, hacky R code.  Programming R in an elegant, functional way is not more difficult, but is immediately less intuitive to people who were brought up reading and writing imperative code.  I would always recommend people learning R to learn these functional concepts from the outset because this way you are working with how the language was designed, rather than against it.&lt;/p&gt;

&lt;p&gt;To show just how functional a language is at its core, it is first important to recognise that everything in R is a function call, even if it looks like it isn&amp;#39;t. So, &lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;&amp;gt; 1 + 2
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] 3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;hellip; is exactly the same as&amp;hellip;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;&amp;gt; `+`(1, 2)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;## [1] 3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;+&lt;/code&gt; operator is really just &amp;ldquo;syntactic sugar&amp;rdquo; for a &lt;code&gt;+&lt;/code&gt; function with the arguments 1 and 2 applied to it. Similarly, &lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;&amp;gt; 1:10
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##  [1]  1  2  3  4  5  6  7  8  9 10
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;hellip; is the same as&amp;hellip;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;&amp;gt; `:`(1, 10)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##  [1]  1  2  3  4  5  6  7  8  9 10
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;here again, to give the range of numbers between 1 and 10, &lt;code&gt;:&lt;/code&gt; is really just a function in disguise.  If you were to break down more complex expressions in this way, the result would be code that looks very Scheme-like indeed.&lt;/p&gt;

&lt;p&gt;I will now look in more depth at the functional concepts that are most important in R, Vectorised functions, higher order functions and closures. &lt;/p&gt;

&lt;h3&gt;Vectorised functions&lt;/h3&gt;

&lt;p&gt;Probably the best known FP concept in R is the vectorised function which &amp;#39;automagically&amp;#39; operates on a data structure like a vector in just the same way as on a single number.  Some of the most important of these are the &lt;a href=&quot;http://rpubs.com/daspringate/subsetting&quot;&gt;vector subsetting&lt;/a&gt; operations. In these, you take a declarative approach to programming: you tell R what you want, not how to get it.  Because of this property of operating across vectors, proper use of vectorised functions will drastically reduce the number of loops you have to hand code into your scripts.  They are still performing loops under the hood, but these loops are implemented in C, so are many times faster than a standard for loop in R.  &lt;/p&gt;

&lt;p&gt;For example, when I was first using R for data management and analysis, I spent months writing code like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;&amp;gt; # Get all even numbers up to 200000
&amp;gt; # using S-style vector allocation:
&amp;gt; x &amp;lt;- c()
&amp;gt; for(i in 1:200000){
&amp;gt;     if(i %% 2 == 0){
&amp;gt;         x &amp;lt;- c(x, i)
&amp;gt;     }
&amp;gt; }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is about the worst possible way of achieving the given task (here, getting all even numbers up to 200000).  You are running a &lt;code&gt;for&lt;/code&gt; loop, which is slow in itself, and testing if &lt;code&gt;i&lt;/code&gt; is even on each iteration and then growing the &lt;code&gt;x&lt;/code&gt; vector every time you find that it is.  The code is ugly, slow and verbose (On my machine it took around 10 seconds).&lt;/p&gt;

&lt;p&gt;For me, writing vectorised code was a real revelation.  To achieve the same goal as the code above in a vectorised style:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;&amp;gt; # FP style vectorised operation
&amp;gt; a &amp;lt;- 1:200000
&amp;gt; x &amp;lt;- a[a %% 2 == 0] 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You assign a vector with all the values from 1 t0 200000, then you say &amp;ldquo;I want all of these that are divisible by two&amp;rdquo;.  This ran 3 orders of magnitude faster than the non-FP code, is half the length and clearer - you don&amp;#39;t have to mentally run through the loop in order to work out what it does.  So you get the benefits of both concision (Number of bugs correlate well with lines of code) and clarity (The code becomes almost self-documenting).&lt;/p&gt;

&lt;p&gt;This is a slightly unfair comparison and there are ways to speed up your loops, for example by pre-allocating a vector of the correct length before you run the loop.  However, even if you do this, the result will still be around 20 times slower and will be even more verbose.  It is good practice whenever you write a &lt;code&gt;for&lt;/code&gt; loop in R to check if there is not a better way to do so using vectorised functions. The majority of R&amp;#39;s built-in functions are vectorised and using these effectively is a prerequisite of using R effectively.&lt;/p&gt;

&lt;h3&gt;Higher-order functions&lt;/h3&gt;

&lt;p&gt;Because functions in R are first class citizens of the language, it is trivial to write and use functions that take other functions as arguments.  The most well used of these are the functions in the apply family (&lt;code&gt;lapply&lt;/code&gt;, &lt;code&gt;sapply&lt;/code&gt;, &lt;code&gt;apply&lt;/code&gt; etc.).  These cause a lot of headaches for new-ish R users, who have just got to grips with for loops, but they are really no more difficult to use.  When you use one of these apply functions, you are just taking a collection of data (say a list or vector) and applying the input function to every element of the collection in turn, and collecting the results in another collection.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://daspringate.github.io/img/apply_functions.png&quot; alt=&quot;Apply functions&quot;/&gt;&lt;/p&gt;

&lt;p&gt;Because the mapping of each element to the function is independent of the elements around it, you can split the collections up and join them together at the end, which means that the functions can be better optimised than a &lt;code&gt;for&lt;/code&gt; loop (especially in the case of &lt;code&gt;lapply&lt;/code&gt;) and also easily run over multiple processor cores (see &lt;code&gt;multicore::mclapply&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;Conceptually, to use these functions you just need to think about what your input is, what the function you want to apply to each element is and what data structure you want as your output:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;lapply&lt;/code&gt; : Any collection -&amp;gt; FUNCTION -&amp;gt; list &lt;/li&gt;
&lt;li&gt;&lt;code&gt;sapply&lt;/code&gt; : Any collection -&amp;gt; FUNCTION -&amp;gt; matrix/vector&lt;/li&gt;
&lt;li&gt;&lt;code&gt;apply&lt;/code&gt;  : Matrix/dataframe + margin -&amp;gt; FUNCTION -&amp;gt; matrix/vector&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Reduce&lt;/code&gt; : Any collection -&amp;gt; FUNCTION -&amp;gt; single element&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;so if you want your output in a list, use &lt;code&gt;lapply&lt;/code&gt;. If you want a vector or matrix, use &lt;code&gt;sapply&lt;/code&gt;.  If you want to calculate summaries of the rows or columns of a dataframe, use &lt;code&gt;apply&lt;/code&gt;. If you want to condense your dataset down into a single summary number, use &lt;code&gt;Reduce&lt;/code&gt;.  There are several other functions in the same family, which all follow a similar pattern.&lt;/p&gt;

&lt;h3&gt;Closures&lt;/h3&gt;

&lt;p&gt;Closures are at the heart of all functional programming languages.  Essentially a closure is a function to which has been added data via its arguments.  The function &amp;#39;closes over&amp;#39; the data at the time the function was created and it is possible to access it at a later time.  Compare this to the idea of an object in languages like C++ and Java, which are data with functions attached to them.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://daspringate.github.io/img/closure.png&quot; alt=&quot;closures&quot;/&gt;&lt;/p&gt;

&lt;p&gt;You can use closures to build wrappers around functions with new default values and partially apply functions and even mimic Object-oriented style objects, but possibly most interestingly, you can build functions that return other functions.  This is great if you want to call a function many times on the same dataset but with different parameters, such as in maximum-likelihood optimisation problems when you are seeking to minimise some cost function and also for randomisation and bootstrapping algorithms.&lt;/p&gt;

&lt;p&gt;To demonstrate the usefulness of this, I am now going to build a generic bootstrapping algorithm in a functional style that can be applied to any linear model.  It will demonstrate not only functions returning functions, but higher-order functions (in this case, &lt;code&gt;sapply&lt;/code&gt;), anonymous functions (in the mapping function to &lt;code&gt;sapply&lt;/code&gt;) and vectorised functions.  I will then compare this against a non-FP version of the algorithm and hopefully some of the advantages of writing in an FP style in R will become clear.  Here is the code. I am doing a bootstrap of a simple linear model on the classic &lt;code&gt;iris&lt;/code&gt; dataset:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;boot.lm &amp;lt;- function(formula, data, ...){
  function(){
    lm(formula=formula, 
       data=data[sample(nrow(data), replace=TRUE),], ...)
  }
}

iris.boot &amp;lt;- boot.lm(Sepal.Length ~ Petal.Length, iris)
bstrap &amp;lt;- sapply(X=1:1000, 
                 FUN=function(x) iris.boot()$coef)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That is the algorithm. The boot.lm function returns a closure.  You pass it a linear model formula and a dataframe and it returns a function with no arguments that itself returns a linear model object of a bootstrapped replicate (sample with replacement) of the supplied data.  So, the iris.boot function takes the formula of Sepal.Length~Petal.Length and the iris dataset and every time you call it it gives a new bootstrap replicate of that model on that data.  You then just need to run this 1000 times and collect the coefficients, which can be done with a one-liner sapply call.  We are using sapply because we want a matrix of coefficients with one line per replicate. The &lt;code&gt;FUN&lt;/code&gt; argument to &lt;code&gt;sapply&lt;/code&gt; is an anonymous function that returns the coefficients of the function.  You could have equally well have written something like&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;get.coefs &amp;lt;- function(x){
    iris.boot$coef
}

bstrap &amp;lt;- sapply(X=1:1000, 
                 FUN=get.coefs)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;hellip; but because the function is so short, it is no less clear to include it without a name.&lt;/p&gt;

&lt;p&gt;Once the model has run, we can use the &lt;code&gt;apply&lt;/code&gt; higher-order function to summarise the rows of &lt;code&gt;bstrap&lt;/code&gt; by applying the &lt;code&gt;quantile&lt;/code&gt; function to give the median and 95% confidence intervals:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;
apply(bstrap, MARGIN=1, FUN=quantile, 
      probs=c(0.025, 0.5, 0.975))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##       (Intercept) Petal.Length
## 2.5%        4.157       0.3689
## 50%         4.309       0.4083
## 97.5%       4.459       0.4444
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is an elegant way to solve a common analysis problem in R.  If you are running a large model and you want to speed things up (and you have a few cores free!), it is a simple task and a couple of lines of code to replace the call to &lt;code&gt;sapply&lt;/code&gt; to one to &lt;code&gt;multicore::mclapply&lt;/code&gt; and run the model on as many processor cores as you can.&lt;/p&gt;

&lt;p&gt;In contrast, here is a roughly equivalent non-FP style bootstrapping algorithm:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;boot_lm_nf &amp;lt;- function(d, form, iters, output, ...){
  for(i in 1:iters){
    x &amp;lt;- lm(formula=form, 
            data=d[sample(nrow(d),
                   replace = TRUE),], ...)[[output]]
    if(i == 1){
      bootstrap &amp;lt;- matrix(data=NA, nrow=iters, 
                    ncol=length(x), 
                    dimnames=list(NULL,names(x)))
      bootstrap[i,] &amp;lt;- x
    } else bootstrap[i,] &amp;lt;- x
  }
  bootstrap
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This ugly beast is full of &lt;code&gt;for&lt;/code&gt;s and &lt;code&gt;if&lt;/code&gt;s and braces and brackets and double brackets.  It has a load of extra boilerplate code to define the variables and fill the matrices.  Plus, it is less generic than the FP version since you can only output the attributes of the model itself, whereas previously we could apply any function we like in place of the anonymous function in the &lt;code&gt;sapply&lt;/code&gt; call.  It is more than twice as verbose and impossible to multicore without a complete rewrite.  On top of all that, getting the coefficients out in a non-FP way is a tedious task:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;r&quot;&gt;bstrap2 &amp;lt;- boot_lm_nf(d=iris, 
            form=Sepal.Length ~ Petal.Length, 
            iters=1000, output=&amp;quot;coefficients&amp;quot;)
CIs &amp;lt;- c(0.025, 0.5, 0.975)
cbind( &amp;quot;(Intercept)&amp;quot;=quantile(bstrap2[,1],probs = CIs),
      &amp;quot;Petal.Length&amp;quot;=quantile(bstrap2[,2],probs = CIs))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;##       (Intercept) Petal.Length
## 2.5%        4.153       0.3708
## 50%         4.304       0.4094
## 97.5%       4.445       0.4480
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The code duplication in the &lt;code&gt;cbind&lt;/code&gt; is a pain, as is having to name the coefficients directly.  Both of these reduce the generalisability of the algorithm.&lt;/p&gt;

&lt;h2&gt;Wrapping up&lt;/h2&gt;

&lt;p&gt;I hope I have demonstrated that writing more functional R code is &lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;More concise (fewer lines of code)&lt;/li&gt;
&lt;li&gt;Often faster (Particularly with effective vectorisation)&lt;/li&gt;
&lt;li&gt;Clearer and less prone to bugs (because you are abstracting away a lot of the &amp;#39;how to&amp;#39; code)&lt;/li&gt;
&lt;li&gt;More elegant&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;R is a strongly functional language to its core and if you work with this in your code, your R hacking will be more intuitive, productive and enjoyable.&lt;/p&gt;

&lt;h2&gt;Further Reading&lt;/h2&gt;

&lt;p&gt;Here are some good and accessible resources available if you want to learn more about functional programming in general and FP in R in particular:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;mitpress.mit.edu/sicp&quot;&gt;Structure and interpretation of computer programs&lt;/a&gt; by Abelson and Sussman is the bible of FP and is written by the creators of Scheme.  This book has been used as the core of the MIT Computer Science course since the early 1990s and is still not dated.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;github.com/hadley/devtools/wiki&quot;&gt;Hadley Wickham&amp;#39;s in progress ebook&lt;/a&gt; on Github is a fantastic resource on FP in R amongst a host of other advanced R topics.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;www.burns-stat.com/pages/Tutor/R_inferno.pdf&quot;&gt;The R Inferno&lt;/a&gt; by Patrick Burns is a classic free online book on R and has a great chapter on vectorisation and when it is best to apply it.&lt;/li&gt;
&lt;li&gt;If you are intersted in the metaphysical stuff at the start of this post, Rich Hickey, the inventor of the Clojure language give &lt;a href=&quot;http://www.infoq.com/presentations/Are-We-There-Yet-Rich-Hickey&quot;&gt;this&lt;/a&gt; great talk on the importance of FP and the failings of the traditional OOP model.  The talk was also summarised nicely in &lt;a href=&quot;http://www.flyingmachinestudios.com/programming/the-unofficial-guide-to-rich-hickeys-brain/&quot;&gt;this&lt;/a&gt; blog post.&lt;/li&gt;
&lt;/ul&gt;
</description></item>
    <item><title>Two R tutorials for beginners</title><link>http://daspringate.github.io/posts/2013/04/beginners_tutorials.html</link><category>R</category><category>tutorials</category><category>subsetting</category><category>vectorisation</category><description>&lt;h1&gt;Two R tutorials for beginners&lt;/h1&gt;

&lt;p&gt;I am currently in the process of rescuing some of the pages from my now defunct datajujitsu.co.uk blogger blog and moving to this Github/Clojure/Bootstrap version.  I also today gave a tutorial to the University of Manchester on data cleaning and subsetting, so I am killing two birds with one stone by linking to the &lt;a href=&quot;http://rpubs.com/&quot;&gt;Rpubs&lt;/a&gt; pages for both this and a short tutorial I gave last year on vectorisation.&lt;/p&gt;

&lt;p&gt;The tutorials are:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&quot;http://rpubs.com/daspringate/subsetting&quot;&gt;Subsetting in R: Spring cleaning your data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://rpubs.com/daspringate/vectorisation&quot;&gt;Speeding up your R code - vectorisation tricks for beginners&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The R markdown source file for both of these are available on &lt;a href=&quot;https://github.com/DASpringate/tutorials&quot;&gt;my github page&lt;/a&gt;. Rpubs is a great site from the people behind RStudio that allows you to upload R markdown scripts compiled using Knitr in no time at all.  &lt;/p&gt;

&lt;p&gt;Using R Markdown, Knitr, RStudio and Rpubs to produce and publish tutorials has proved a complete joy.  It is simple, quick and painless to get pages online with embedded R code and output.  &lt;/p&gt;

&lt;p&gt;I have also produced a slide presentation for an internal seminar series in my department using R Markdown, Knitr, Pandoc and Beamer. I was really pleased with the results (Which are also on my &lt;a href=&quot;https://github.com/DASpringate/tutorials/tree/master/Primary_Care_Databases/March_2013&quot;&gt;github page&lt;/a&gt;) and how easily I was able to achieve them, particularly with  the huge reduction of Latex boilerplate I was forced to write.  I will be doing all of my presentations with this method in future and will blog about the workflow for doing so in due course.&lt;/p&gt;
</description></item>
    <item><title>Scraping organism metadata for Treebase repositories from GOLD using Python and R</title><link>http://daspringate.github.io/posts/2013/04/scraping_metadata.html</link><category>R</category><category>Python</category><category>Bioinformatics</category><description>&lt;h1&gt;Scraping organism metadata for Treebase repositories from GOLD using Python and R&lt;/h1&gt;

&lt;p&gt;I recently wanted to get hold of habitat/phenotype/sequencing metadata for the individual organisms of  an archived &lt;a href=&quot;http://treebase.org/treebase-web/home.html&quot;&gt;Treebase&lt;/a&gt; project.)&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;http://www.genomesonline.org/&quot;&gt;GOLD&lt;/a&gt; database holds  more than 18000 full genomes.  For many of these it provides pretty good metadata (&lt;a href=&quot;http://genomesonline.org/cgi-bin/GOLD/bin/GOLDCards.cgi?goldstamp=Gc00536&quot;&gt;GOLDcards&lt;/a&gt;) which are indirectly linked  to Treebase via &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/taxonomy&quot;&gt;NCBI&lt;/a&gt; taxa &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/Taxonomy/Browser/wwwtax.cgi?mode=Info&amp;amp;amp;id=122368&quot;&gt;IDs&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Unfortunately GOLD does not seem to have any kind of API for systematic downloads, so I hacked together a very &lt;a href=&quot;https://gist.github.com/2929217&quot;&gt;quick-and-dirty scraper&lt;/a&gt; in Python that reads in taxa from a Treebase repo, follows the links to each species NCBI page and downloads the linked GOLDcard, if it exists.&lt;/p&gt;

&lt;p&gt;Here is the code. You will need the external BeautifulSoup and lxml libraries for this to work - both are fantastic. (The Treebase repo here is from Wu et al. 2009**, just change the url string for a different repo&amp;hellip;):&lt;/p&gt;

&lt;p&gt;Once you have downloaded all of the available files, It would be great to have your metadata in a nice flatfile with one line per taxa, right?  I did this with a little &lt;a href=&quot;https://gist.github.com/2929366&quot;&gt;R script&lt;/a&gt; using the rather wonderful readHTMLtable() function in the XML (install.packages(&amp;#39;XML&amp;#39;)) package. &lt;/p&gt;

&lt;p&gt;The output is a semicolon separated file with taxa in the rows and the different categories of metadata in columns. The metadata is often fairly incomplete, and  there are plenty of omissions, but hopefully it will become more useful as more deposits are made to GOLD.&lt;/p&gt;

&lt;p&gt;** Wu D., Hugenholtz P., Mavromatis K., Pukall R., Dalin E., Ivanova N.N., Kunin V., Goodwin L., Wu M., Tindall B.J., Hooper S.D., Pati A., Lykidis A., Spring S., Anderson I.J., D’haeseleer P., Zemla A., Singer M., Lapidus A., Nolan M., Copeland A., Chen F., Cheng J., Lucas S., Kerfeld C., Lang E., Gronow S., Chain P., Bruce D., Rubin E.M., Kyrpides N.C., Klenk H., &amp;amp; Eisen J.A. 2009. A phylogeny-driven genomic encyclopaedia of Bacteria and Archaea. Nature, 462(7276): 1056-1060.&lt;/p&gt;
</description></item>
  </channel>
</rss>